{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 目录\n",
    "- [面向初学者的快速入门](#面向初学者的快速入门)\n",
    "- [面向专家的快速入门](#面向专家的快速入门)\n",
    "- [tensorflow2改变了什么](#tensorflow2改变了什么)\n",
    "- [推荐使用技巧](#推荐使用技巧)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 面向初学者的快速入门\n",
    "\n",
    "源自：https://www.tensorflow.org/tutorials/quickstart/beginner\n",
    "\n",
    "[回到目录](#目录)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0-alpha0\n",
      "2.2.4-tf\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 4s 72us/sample - loss: 0.3001 - accuracy: 0.9122\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 0.1472 - accuracy: 0.9564\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 4s 71us/sample - loss: 0.1090 - accuracy: 0.9675\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 0.0883 - accuracy: 0.9726\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 0.0754 - accuracy: 0.9764\n",
      "10000/10000 - 0s - loss: 0.0718 - accuracy: 0.9774\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.07179369834843091, 0.9774]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "# 安装 TensorFlow\n",
    "\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)         # 2.0.0-alpha0\n",
    "print(tf.keras.__version__)   # 2.2.4-tf\n",
    "\n",
    "# 载入并准备好 MNIST 数据集。将样本从整数转换为浮点数\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "# 将模型的各层堆叠起来，以搭建 tf.keras.Sequential 模型。为训练选择优化器和损失函数\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# 训练并验证\n",
    "model.fit(x_train, y_train, epochs=5)\n",
    "\n",
    "model.evaluate(x_test,  y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 面向专家的快速入门\n",
    "\n",
    "源自：https://www.tensorflow.org/tutorials/quickstart/advanced\n",
    "\n",
    "[回到目录](#目录)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "# 加载并准备 MNIST 数据集\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "# Add a channels dimension:(x,y,z) => (x,y,z,1)\n",
    "x_train = x_train[..., tf.newaxis]\n",
    "x_test = x_test[..., tf.newaxis]\n",
    "\n",
    "# 使用 tf.data 来将数据集切分为 batch 以及混淆数据集\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(10000).batch(32)\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(32)\n",
    "\n",
    "# 使用 Keras 模型子类化（model subclassing） API 构建 tf.keras 模型\n",
    "class MyModel(Model):\n",
    "  def __init__(self):\n",
    "    super(MyModel, self).__init__()\n",
    "    self.conv1 = Conv2D(32, 3, activation='relu')\n",
    "    self.flatten = Flatten()\n",
    "    self.d1 = Dense(128, activation='relu')\n",
    "    self.d2 = Dense(10, activation='softmax')\n",
    "\n",
    "  def call(self, x):\n",
    "    x = self.conv1(x)\n",
    "    x = self.flatten(x)\n",
    "    x = self.d1(x)\n",
    "    return self.d2(x)\n",
    "\n",
    "model = MyModel()\n",
    "\n",
    "\n",
    "# 为训练选择优化器与损失函数\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "# 选择衡量指标来度量模型的损失值（loss）和准确率（accuracy）。这些指标在 epoch 上累积值，然后打印出整体结果\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.13203899562358856, Accuracy: 96.05833435058594, Test Loss: 0.06168621405959129, Test Accuracy: 97.90999603271484\n",
      "Epoch 2, Loss: 0.085730642080307, Accuracy: 97.43499755859375, Test Loss: 0.05973690375685692, Test Accuracy: 97.95500183105469\n",
      "Epoch 3, Loss: 0.06337665021419525, Accuracy: 98.0999984741211, Test Loss: 0.05973971262574196, Test Accuracy: 98.05333709716797\n",
      "Epoch 4, Loss: 0.05047867074608803, Accuracy: 98.4808349609375, Test Loss: 0.060349710285663605, Test Accuracy: 98.12249755859375\n",
      "Epoch 5, Loss: 0.04229719936847687, Accuracy: 98.72100067138672, Test Loss: 0.06144608557224274, Test Accuracy: 98.15800476074219\n"
     ]
    }
   ],
   "source": [
    "# 使用 tf.GradientTape 来训练模型\n",
    "\n",
    "@tf.function\n",
    "def train_step(images, labels):\n",
    "  with tf.GradientTape() as tape:\n",
    "    predictions = model(images)\n",
    "    loss = loss_object(labels, predictions)\n",
    "  gradients = tape.gradient(loss, model.trainable_variables)\n",
    "  optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "  train_loss(loss)\n",
    "  train_accuracy(labels, predictions)\n",
    "\n",
    "\n",
    "# 测试模型\n",
    "@tf.function\n",
    "def test_step(images, labels):\n",
    "  predictions = model(images)\n",
    "  t_loss = loss_object(labels, predictions)\n",
    "\n",
    "  test_loss(t_loss)\n",
    "  test_accuracy(labels, predictions)\n",
    "\n",
    "EPOCHS = 5\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "  for images, labels in train_ds:\n",
    "    train_step(images, labels)\n",
    "\n",
    "  for test_images, test_labels in test_ds:\n",
    "    test_step(test_images, test_labels)\n",
    "\n",
    "  template = 'Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'\n",
    "  print (template.format(epoch+1,\n",
    "                         train_loss.result(),\n",
    "                         train_accuracy.result()*100,\n",
    "                         test_loss.result(),\n",
    "                         test_accuracy.result()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 问题：为什么用了```tf.GradientTape()```？ \n",
    "[解答位于：推荐使用技巧 — eager执行]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tensorflow2改变了什么\n",
    "\n",
    "源自：https://www.tensorflow.org/guide/effective_tf2\n",
    "\n",
    "[回到目录](#目录)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **API Cleanup**\n",
    "\n",
    "重整了很多混乱的模块\n",
    "\n",
    "- **Eager execution**\n",
    "\n",
    "TensorFlow虽是深度学习领域最广泛使用的框架，但是对比PyTorch这一动态图框架，采用静态图（Graph模式）的TensorFlow确实是难用。\n",
    "\n",
    "好在最近TensorFlow支持了eager模式，对标PyTorch的动态执行机制。\n",
    "\n",
    "```python\n",
    "# 在tensorflow2中默认使用Eager Execution\n",
    "tf.executing_eagerly()\n",
    "```\n",
    "\n",
    "\n",
    "- **No more globals**\n",
    "\n",
    "变量的定义更加便捷，不像 tensorflow 1.X 那么复杂\n",
    "\n",
    "- **Functions, not sessions**\n",
    "```python\n",
    "# TensorFlow 1.X\n",
    "outputs = session.run(f(placeholder), feed_dict={placeholder: input})\n",
    "# TensorFlow 2.0\n",
    "outputs = f(input)\n",
    "```\n",
    "\n",
    "**AutoGraph** converts a subset of Python constructs into their TensorFlow equivalents:\n",
    "```python\n",
    "for/while -> tf.while_loop (break and continue are supported)\n",
    "if -> tf.cond\n",
    "for _ in dataset -> dataset.reduce\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 推荐使用技巧\n",
    "\n",
    "- [eager执行](#1.eager执行)\n",
    "- [AutoGraph](#2.AutoGraph)\n",
    "- [tf.function性能优化](#3.tf.function性能优化)\n",
    "- [tf.keras模型构建](#4.tf.keras模型构建)\n",
    "- [模型训练](#5.模型训练)\n",
    "\n",
    "源自：\n",
    "\n",
    "https://www.tensorflow.org/guide/effective_tf2\n",
    "\n",
    "https://mp.weixin.qq.com/s/1I9QR-xoum0IfofsRM1c5A\n",
    "\n",
    "https://mp.weixin.qq.com/s/5ygXEMr7DAEPN_G1V_Qkug\n",
    "\n",
    "\n",
    "[回到目录](#目录)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.eager执行\n",
    "\n",
    "TensorFlow的Eager执行时一种命令式编程（imperative programming），这和原生Python是一致的，当你执行某个操作时是立即返回结果的。\n",
    "\n",
    "而TensorFlow一直是采用Graph模式，即先构建一个计算图，然后需要开启Session，喂进实际的数据才真正执行得到结果。显然，**eager执行更简洁，我们可以更容易debug自己的代码，这也是为什么PyTorch更简单好用的原因**。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[4. 6.]\n",
      " [4. 6.]], shape=(2, 2), dtype=float32)\n",
      "------------\n",
      "[[4. 6.]\n",
      " [4. 6.]]\n"
     ]
    }
   ],
   "source": [
    "x = tf.ones((2, 2), dtype=tf.dtypes.float32)\n",
    "y = tf.constant([[1, 2],\n",
    "                 [3, 4]], dtype=tf.dtypes.float32)\n",
    "z = tf.matmul(x, y)\n",
    "print(z)\n",
    "print(\"------------\")\n",
    "print(z.numpy())    #获得Tensor所对应的numpy数组"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 可以直接看到结果\n",
    "\n",
    "上面例子可以看到在eager执行下，每个操作后的返回值是tf.Tensor，其包含具体值，不再像Graph模式下那样只是一个计算图节点的符号句柄。\n",
    "\n",
    "由于可以立即看到结果，这非常有助于程序debug。更进一步地，```调用tf.Tensor.numpy()方法可以获得Tensor所对应的numpy数组```。\n",
    "\n",
    "#### 1.2 可以使用Python原生功能\n",
    "\n",
    "```python\n",
    "random_value = tf.random.uniform([], 0, 1)\n",
    "x = tf.reshape(tf.range(0, 4), [2, 2])\n",
    "print(random_value)\n",
    "if random_value.numpy() > 0.5:\n",
    "    y = tf.matmul(x, x)\n",
    "else:\n",
    "    y = tf.add(x, x)\n",
    "    \n",
    "# tf.Tensor(0.6259608, shape=(), dtype=float32)\n",
    "```\n",
    "\n",
    "这种动态控制流主要得益于eager执行得到Tensor可以取出numpy值，这避免了使用Graph模式下的tf.cond和tf.while等算子。\n",
    "\n",
    "#### 1.3 tf.GradientTape()是什么？\n",
    "\n",
    "**一个重要的问题，在egaer模式下如何计算梯度。** \n",
    "\n",
    "在Graph模式时，我们在构建模型前向图时，同时也会构建梯度图，这样实际喂数据执行时可以很方便计算梯度。\n",
    "\n",
    "但是eager执行是动态的，这就需要每一次执行都要记录这些操作以计算梯度，这是通过tf.GradientTape来追踪所执行的操作以计算梯度\n",
    "\n",
    "```python\n",
    "w = tf.Variable([[1.0]])\n",
    "with tf.GradientTape() as tape:\n",
    "  loss = w * w + 3. * w + 8.\n",
    "grad = tape.gradient(loss, w)\n",
    "print(grad)\n",
    "\n",
    "# tf.Tensor([[5.]], shape=(1, 1), dtype=float32)\n",
    "```\n",
    "\n",
    "对于eager执行，每个tape会记录当前所执行的操作，这个tape只对当前计算有效，并计算相应的梯度。\n",
    "\n",
    "PyTorch也是动态图模式，但是与TensorFlow不同，它是每个需要计算Tensor会拥有```grad_fn```以追踪历史操作的梯度。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.AutoGraph\n",
    "\n",
    "TensorFlow 2.0引入的eager提高了代码的简洁性，而且更容易debug。\n",
    "\n",
    "> 但是对于性能来说，**eager执行相比Graph模式会有一定的损失**。这不难理解，毕竟原生的Graph模式是先构建好静态图，然后才真正执行。这对于在分布式训练、性能优化和生产部署方面具有优势。\n",
    "但是好在，**TensorFlow 2.0引入了tf.function和AutoGraph来缩小eager执行和Graph模式的性能差距，其核心是将一系列的Python语法转化为高性能的graph操作**。\n",
    "\n",
    "AutoGraph在TensorFlow 1.x已经推出，```主要是可以将一些常用的Python代码转化为TensorFlow支持的Graph代码```。\n",
    "\n",
    "一个典型的例子是在TensorFlow中我们必须使用```tf.while和tf.cond```等复杂的算子来实现动态流程控制。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eager results: 81.00, 0.00\n",
      "Graph results: 81.00, 0.00\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "但是现在我们可以使用Python原生的for和if等语法写代码，然后采用AutoGraph转化为TensorFlow所支持的代码。\n",
    "'''\n",
    "def square_if_positive(x):\n",
    "    if x > 0:\n",
    "        x = x * x\n",
    "    else:\n",
    "        x = 0.0\n",
    "    return x\n",
    "\n",
    "# eager 模式\n",
    "print('Eager results: %2.2f, %2.2f' % (square_if_positive(tf.constant(9.0)),\n",
    "                                       square_if_positive(tf.constant(-9.0))))\n",
    "# graph 模式\n",
    "tf_square_if_positive = tf.autograph.to_graph(square_if_positive)\n",
    "with tf.Graph().as_default():\n",
    "  # The result works like a regular op: takes tensors in, returns tensors.\n",
    "  # You can inspect the graph using tf.get_default_graph().as_graph_def()\n",
    "    g_out1 = tf_square_if_positive(tf.constant( 9.0))\n",
    "    g_out2 = tf_square_if_positive(tf.constant(-9.0))\n",
    "    with tf.compat.v1.Session() as sess:\n",
    "        print('Graph results: %2.2f, %2.2f\\n' % (sess.run(g_out1), sess.run(g_out2)))\n",
    "\n",
    "# 上面我们定义了一个square_if_positive函数，它内部使用的Python的原生的if语法，对于TensorFlow 2.0的eager执行，这是没有问题的。\n",
    "# 然而这是TensorFlow 1.x所不支持的，但是使用AutoGraph可以将这个函数转为Graph函数，你可以将其看成一个常规TensorFlow op，其可以在Graph模式下运行（tf2 没有Session，这是tf1.x的特性，想使用tf1.x的话需要调用tf.compat.v1）。\n",
    "# 大家要注意eager模式和Graph模式的差异，尽管结果是一样的，但是Graph模式更高效。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from __future__ import print_function\n",
      "\n",
      "def tf__square_if_positive(x):\n",
      "  try:\n",
      "    with ag__.function_scope('square_if_positive'):\n",
      "      do_return = False\n",
      "      retval_ = None\n",
      "      cond = ag__.gt(x, 0)\n",
      "\n",
      "      def if_true():\n",
      "        with ag__.function_scope('if_true'):\n",
      "          x_1, = x,\n",
      "          x_1 = x_1 * x_1\n",
      "          return x_1\n",
      "\n",
      "      def if_false():\n",
      "        with ag__.function_scope('if_false'):\n",
      "          x = 0.0\n",
      "          return x\n",
      "      x = ag__.if_stmt(cond, if_true, if_false)\n",
      "      do_return = True\n",
      "      retval_ = x\n",
      "      return retval_\n",
      "  except:\n",
      "    ag__.rewrite_graph_construction_error(ag_source_map__)\n",
      "\n",
      "\n",
      "\n",
      "tf__square_if_positive.autograph_info__ = {}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "从本质上讲，AutoGraph是将Python代码转为TensorFlow原生的代码，我们可以进一步看到转化后的代码\n",
    "'''\n",
    "print(tf.autograph.to_code(square_if_positive))\n",
    "\n",
    "# 可以看到AutoGraph转化的代码定义了两个条件函数，然后调用if_stmt op，应该就是类似tf.cond的op。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eager result: 42\n",
      "Graph result: 42\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "AutoGraph支持很多Python特性，比如循环.\n",
    "对于大部分Python特性AutoGraph是支持的，但是其仍然有限制，具体可以见Capabilities and Limitations。\n",
    "https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/LIMITATIONS.md\n",
    "'''\n",
    "def sum_even(items):\n",
    "    s = 0\n",
    "    for c in items:\n",
    "        if c % 2 > 0:\n",
    "            continue\n",
    "        s += c\n",
    "    return s\n",
    "\n",
    "print('Eager result: %d' % sum_even(tf.constant([10,12,15,20])))\n",
    "\n",
    "tf_sum_even = tf.autograph.to_graph(sum_even)\n",
    "with tf.Graph().as_default(), tf.compat.v1.Session() as sess:\n",
    "    print('Graph result: %d\\n' % sess.run(tf_sum_even(tf.constant([10,12,15,20]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eager at orginal code: 0.1542729390785098\n",
      "Eager at autograph code: 0.3119836784899235\n",
      "Graph at autograph code: 0.05139109306037426\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "此外，要注意的一点是，经过AutoGraph转换的新函数是可以eager模式下执行的，但是性能却并不会比转换前的高.\n",
    "'''\n",
    "import timeit\n",
    "\n",
    "x = tf.constant([10, 12, 15, 20])\n",
    "print(\"Eager at orginal code:\", timeit.timeit(lambda: sum_even(x), number=100))\n",
    "print(\"Eager at autograph code:\", timeit.timeit(lambda: tf_sum_even(x), number=100))\n",
    "with tf.Graph().as_default(), tf.compat.v1.Session() as sess:\n",
    "    graph_op = tf_sum_even(tf.constant([10, 12, 15, 20]))\n",
    "    sess.run(graph_op)  # remove first call\n",
    "    print(\"Graph at autograph code:\", timeit.timeit(lambda: sess.run(graph_op), number=100))\n",
    "    \n",
    "# 从结果上看，Graph模式下的执行效率是最高的，原来的代码在eager模式下效率次之，经AutoGraph转换后的代码效率最低。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.tf.function性能优化\n",
    "\n",
    "尽管eager执行更简洁，但是Graph模式却是性能更高，为了减少这个性能gap，TensorFlow 2.0引入了tf.function\n",
    "> 在TensorFlow 2.0，我们一般不会直接使用 `tf.autograph`，因为eager执行下效率没有提升。要真正达到Graph模式下的效率，要依赖 ```tf.function``` 这个更强大的利器。\n",
    "\n",
    "简单来说，就是tf.function可以将一个func中的TensorFlow操作构建为一个Graph，这样在调用时是执行这个Graph，这样计算性能更优.\n",
    "\n",
    "- 3.1 [计算性能更优]()\n",
    "- 3.2 [调用便捷]()\n",
    "- 3.3 [多态性polymorphism]()\n",
    "- 3.4 [指定输入参数类型 input_signature]()\n",
    "- 3.5 [指定参数autograph]()\n",
    "- 3.6 [应用到类方法中]()\n",
    "- 3.7 [tf.print看具体数值]()\n",
    "- 3.8 [dubug时设置tf.config.experimental_run_functions_eagerly=True]()\n",
    "\n",
    "具体内容请看 [A02tf.function](./A02tf.function.ipynb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.tf.keras模型构建\n",
    "\n",
    "在tf.layers以及tf.contrib.slim等高级API来创建模型，但是`2.0仅仅支持tf.keras.layers`，无论如何，省的大家重复造轮子，也意味着模型构建的部分大家都是统一的，增加代码的复用性（回忆一下原来的TensorFlow模型构建真是千奇百怪）。\n",
    "\n",
    "值得注意的tf.nn模块依然存在，里面是各种常用的nn算子，不过大部分人不会去直接用这些算子构建模型，因为keras.layers基本上包含了常用的网络层。\n",
    "\n",
    "- [4.1 采用Keras原有方式]()\n",
    "\n",
    "如 采用`tf.keras.Sequential`\n",
    "\n",
    "```python\n",
    "model = tf.keras.Sequential([\n",
    "layers.Dense(64, activation='relu', input_shape=(32,)),\n",
    "layers.Dense(64, activation='relu'),\n",
    "layers.Dense(10, activation='softmax')])\n",
    "```\n",
    "\n",
    "- [4.2 采用keras的functional API]()\n",
    "\n",
    "```python\n",
    "inputs = keras.Input(shape=(784,), name='img')\n",
    "x = layers.Dense(64, activation='relu')(inputs)\n",
    "x = layers.Dense(64, activation='relu')(x)\n",
    "outputs = layers.Dense(10, activation='softmax')(x)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs, name='mnist_model')\n",
    "```\n",
    "\n",
    "- [4.3 自定义layer&model]()\n",
    "\n",
    "继承`tf.keras.layers.Layer`\n",
    "\n",
    "继承`tf.keras.Model`,创建包含多layers的模块或者模型\n",
    "\n",
    "\n",
    "具体内容请看 [A03模型构建](./A03model-build.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.模型训练\n",
    "\n",
    "具体内容请看 [A04模型训练](A04training.ipynb), [A05分布式训练](A05distribute-training.ipynb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
