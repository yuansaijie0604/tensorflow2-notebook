{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 分布式训练\n",
    "\n",
    "https://www.tensorflow.org/guide/distributed_training\n",
    "\n",
    "https://www.tensorflow.org/tutorials/distribute\n",
    "\n",
    "- [1. 策略类型介绍](#1.策略类型介绍)\n",
    "- [2. 使用Keras进行分布式训练](#2.使用Keras进行分布式训练)\n",
    "- [3. 使用自定义训练循环进行分布式训练](#3.使用自定义训练循环进行分布式训练)\n",
    "- [4. 使用Estimator进行多工作器训练](#4.使用Estimator进行多工作器训练)\n",
    "- [5. 其他](#5.其他)\n",
    "\n",
    "由于这边涉及到Estimator,可以转而了解下 [A08估计器](A08Estimator.ipynb)。tf.estimator中包含很多机器学习的算法，使用方法类似于sklearn。tf.keras.estimator中包含一个函数`model_to_estimator`，实现从Keras模型到Estimator模型。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import tensorflow as tf\n",
    "tf.keras.backend.clear_session()\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow.keras.layers as layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.策略类型介绍\n",
    "\n",
    "Training API | MirroredStrategy | TPUStrategy | MultiWorkerMirroredStrategy | CentralStorageStrategy | ParameterServerStrategy | OneDeviceStrategy\n",
    "---|---|---|---|---|---|---\n",
    "Keras API | Supported | Experimental support | Experimental support | Experimental support | Supported planned post 2.0 | Supported\n",
    "Custom training loop | Experimental support | Experimental support | Support planned post 2.0 | Support planned post 2.0 | No support yet | Supported\n",
    "Estimator API | Limited Support | Not supported | Limited Support | Limited Support | Limited Support | Limited Support\n",
    "\n",
    "- [1.1 镜像策略]()——单机多卡同步训练★★★\n",
    "\n",
    "**这是在一台计算机上的多 GPU（单机多卡）进行同时训练的图形内复制（in-graph replication）。事实上，它会将所有模型的变量复制到每个处理器上，然后，通过使用 all-reduce 去整合所有处理器的梯度（gradients），并将整合的结果应用于所有副本之中。**\n",
    "\n",
    "我们最常用的分布式策略是单机多卡同步训练，tf.distribute.MirroredStrategy完美支持这种策略。这种策略将在每个GPU设备上创建一个模型副本（replica），模型中的参数在所有replica之间映射，称之为MirroredVariables，当他们执行相同更新时将在所有设备间同步。\n",
    "\n",
    "底层的通信采用all-reduce算法，all-reduce方法可以将多个设备上的Tensors聚合在每个设备上，这种通信方式比较高效，而all-reduce算法有多中实现方式，这里默认采用NVIDIA NCCL的all-reduce方法。\n",
    "\n",
    "```python\n",
    "mirrored_strategy = tf.distribute.MirroredStrategy()   # 使用TensorFlow可见的所有GPU，并将NCCL用作跨设备通信。\n",
    "mirrored_strategy = tf.distribute.MirroredStrategy(devices=[\"/gpu:0\", \"/gpu:1\"])  # 这里将在GPU 0和1上同步训练\n",
    "```\n",
    "\n",
    "如果您希望**覆盖跨设备通信**，则可以`cross_device_ops`通过提供参数的实例来使用该参数tf.distribute.CrossDeviceOps。\n",
    "\n",
    "参数cross_device_ops默认为 `tf.distribute.NcclAllReduce()`\n",
    "\n",
    "其他支持的有`tf.distribute.HierarchicalCopyAllReduce`和`tf.distribute.ReductionToOneDevice`\n",
    "\n",
    "\n",
    "- [1.2 中央存储策略]()——单机多卡同步训练，仍在开发中\n",
    "\n",
    "**变量没有被镜像，而是被放置在CPU上，并且操作在所有本地GPU之间复制。如果只有一个GPU，则所有变量和操作都将放置在该GPU上。**\n",
    "\n",
    "```python\n",
    "central_storage_strategy = tf.distribute.experimental.CentralStorageStrategy()\n",
    "```\n",
    "\n",
    "这将创建一个CentralStorageStrategy实例，该实例将使用所有可见的GPU和CPU。在副本上对变量的更新将在应用于变量之前进行汇总。\n",
    "\n",
    "\n",
    "- [1.3 多工镜像策略](#1.3MultiWorkerMirroredStrategy)——多机多卡同步训练，仍在开发中\n",
    "\n",
    "它在所有工作人员的每台设备上的模型中创建所有变量的副本。\n",
    "It implements synchronous distributed training across multiple workers, each with potentially multiple GPUs. Similar to MirroredStrategy, it creates copies of all variables in the model on each device across all workers.\n",
    "\n",
    "```python\n",
    "multiworker_strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()\n",
    "```\n",
    "\n",
    "**集群操作的不同实现**:\n",
    "\n",
    "CollectiveCommunication.RING使用gRPC作为通信层实现基于环的集合。 \n",
    "\n",
    "CollectiveCommunication.NCCL使用Nvidia的NCCL实施集体。 \n",
    "\n",
    "CollectiveCommunication.AUTO将选择推迟到运行时。集体实施的最佳选择取决于GPU的数量和种类以及集群中的网络互连。\n",
    "\n",
    "```python\n",
    "multiworker_strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy(\n",
    "    tf.distribute.experimental.CollectiveCommunication.NCCL)\n",
    "```\n",
    "\n",
    "\n",
    "[回到目录](#分布式训练)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [1.4 TPU策略]()——TPU配置策略，仍在开发中\n",
    "\n",
    "```python\n",
    "cluster_resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=tpu_address)\n",
    "tf.config.experimental_connect_to_cluster(cluster_resolver)\n",
    "tf.tpu.experimental.initialize_tpu_system(cluster_resolver)\n",
    "tpu_strategy = tf.distribute.experimental.TPUStrategy(cluster_resolver)\n",
    "```\n",
    "\n",
    "\n",
    "- [1.5 ParameterServerStrategy]()——多机的参数分布策略\n",
    "\n",
    "支持多台机器上的参数服务器培训。在此设置中，某些机器被指定为工作器，而另一些被指定为参数服务器。模型的每个变量都放在一个参数服务器上。计算在所有工作程序的所有GPU之间复制。\n",
    "```python\n",
    "ps_strategy = tf.distribute.experimental.ParameterServerStrategy()\n",
    "```\n",
    "\n",
    "\n",
    "- [1.6 OneDeviceStrategy]()——单机训练\n",
    "\n",
    "```python\n",
    "strategy = tf.distribute.OneDeviceStrategy(device=\"/gpu:0\")\n",
    "```\n",
    "\n",
    "\n",
    "[回到目录](#分布式训练)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.使用Keras进行分布式训练 \n",
    "\n",
    "第一步：创建 `tf.distribute.Strategy`\n",
    "\n",
    "第二步：把模型的创建编译写在`strategy.scope`的下面.\n",
    "\n",
    "```python\n",
    "mirrored_strategy = tf.distribute.MirroredStrategy()\n",
    "with mirrored_strategy.scope():\n",
    "  model = tf.keras.Sequential([tf.keras.layers.Dense(1, input_shape=(1,))])\n",
    "  model.compile(loss='mse', optimizer='sgd')\n",
    "    \n",
    "# dataset的格式\n",
    "dataset = tf.data.Dataset.from_tensors(([1.], [1.])).repeat(100).batch(10)\n",
    "model.fit(dataset, epochs=2)\n",
    "model.evaluate(dataset)\n",
    "\n",
    "# numpy的格式\n",
    "import numpy as np\n",
    "inputs, targets = np.ones((100, 1)), np.ones((100, 1))\n",
    "model.fit(inputs, targets, epochs=2, batch_size=10)\n",
    "\n",
    "```\n",
    "\n",
    "**在训练具有多个 GPU 的模型时，可以通过增加批量大小（batch size）来有效地使用额外的计算能力。**\n",
    "\n",
    "**通常来说，使用适合 GPU 内存的最大批量大小（batch size），并相应地调整学习速率。**\n",
    "\n",
    "```python\n",
    "BATCH_SIZE_PER_REPLICA = 5\n",
    "global_batch_size = (BATCH_SIZE_PER_REPLICA * mirrored_strategy.num_replicas_in_sync)\n",
    "dataset = tf.data.Dataset.from_tensors(([1.], [1.])).repeat(100)\n",
    "dataset = dataset.batch(global_batch_size)\n",
    "\n",
    "LEARNING_RATES_BY_BATCH_SIZE = {5: 0.1, 10: 0.15}\n",
    "learning_rate = LEARNING_RATES_BY_BATCH_SIZE[global_batch_size]\n",
    "```\n",
    "\n",
    "[回到目录](#分布式训练)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0109 16:21:59.019071 139963976697664 cross_device_ops.py:1168] There is non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of devices: 1\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "# 导入 TensorFlow 和 TensorFlow 数据集 pip3 install tensorflow==2.0.0-beta1\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "tfds.disable_progress_bar()\n",
    "\n",
    "import os\n",
    "\n",
    "# 返回 tf.data 格式的数据集。\n",
    "# 将 with_info 设置为 True 会包含整个数据集的元数据,其中这些数据集将保存在 info 中。 除此之外，该元数据对象包括训练和测试示例的数量。\n",
    "datasets, info = tfds.load(name='mnist', with_info=True, as_supervised=True)\n",
    "mnist_train, mnist_test = datasets['train'], datasets['test']\n",
    "\n",
    "# 提供一个上下文管理器（tf.distribute.MirroredStrategy.scope）来构建你的模型。\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "print('Number of devices: {}'.format(strategy.num_replicas_in_sync))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 处理数据\n",
    "# 执行 info.splits.total_num_examples 来获取总数\n",
    "num_train_examples = info.splits['train'].num_examples\n",
    "num_test_examples = info.splits['test'].num_examples\n",
    "\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "BATCH_SIZE_PER_REPLICA = 64\n",
    "BATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync\n",
    "\n",
    "def scale(image, label):\n",
    "  image = tf.cast(image, tf.float32)\n",
    "  image /= 255\n",
    "\n",
    "  return image, label\n",
    "\n",
    "# 保留了训练数据的内存缓存以提高性能。\n",
    "train_dataset = mnist_train.map(scale).cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "eval_dataset = mnist_test.map(scale).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建模型\n",
    "with strategy.scope():\n",
    "  model = tf.keras.Sequential([\n",
    "      tf.keras.layers.Conv2D(32, 3, activation='relu', input_shape=(28, 28, 1)),\n",
    "      tf.keras.layers.MaxPooling2D(),\n",
    "      tf.keras.layers.Flatten(),\n",
    "      tf.keras.layers.Dense(64, activation='relu'),\n",
    "      tf.keras.layers.Dense(10, activation='softmax')\n",
    "  ])\n",
    "\n",
    "  model.compile(loss='sparse_categorical_crossentropy',\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=['accuracy'])\n",
    "    \n",
    "\n",
    "# 定义回调\n",
    "# 定义检查点（checkpoint）目录以存储检查点（checkpoints）\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# 检查点（checkpoint）文件的名称\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "# 衰减学习率的函数。\n",
    "# 可以定义所需的任何衰减函数\n",
    "def decay(epoch):\n",
    "  if epoch < 3:\n",
    "    return 1e-3\n",
    "  elif epoch >= 3 and epoch < 7:\n",
    "    return 1e-4\n",
    "  else:\n",
    "    return 1e-5\n",
    "\n",
    "# 在每个 epoch 结束时打印LR的回调（callbacks）。\n",
    "class PrintLR(tf.keras.callbacks.Callback):\n",
    "  def on_epoch_end(self, epoch, logs=None):\n",
    "    print('\\nLearning rate for epoch {} is {}'.format(epoch + 1,model.optimizer.lr.numpy()))\n",
    "    \n",
    "callbacks = [\n",
    "    tf.keras.callbacks.TensorBoard(log_dir='./logs'),\n",
    "    tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix,save_weights_only=True),\n",
    "    tf.keras.callbacks.LearningRateScheduler(decay),\n",
    "    PrintLR()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on None steps\n",
      "Epoch 1/12\n",
      "    938/Unknown - 24s 26ms/step - loss: 0.2113 - accuracy: 0.9398\n",
      "Learning rate for epoch 1 is 0.0010000000474974513\n",
      "938/938 [==============================] - 25s 26ms/step - loss: 0.2113 - accuracy: 0.9398\n",
      "Epoch 2/12\n",
      "935/938 [============================>.] - ETA: 0s - loss: 0.0728 - accuracy: 0.9783\n",
      "Learning rate for epoch 2 is 0.0010000000474974513\n",
      "938/938 [==============================] - 12s 13ms/step - loss: 0.0727 - accuracy: 0.9784\n",
      "Epoch 3/12\n",
      "934/938 [============================>.] - ETA: 0s - loss: 0.0503 - accuracy: 0.9851\n",
      "Learning rate for epoch 3 is 0.0010000000474974513\n",
      "938/938 [==============================] - 12s 13ms/step - loss: 0.0503 - accuracy: 0.9851\n",
      "Epoch 4/12\n",
      "933/938 [============================>.] - ETA: 0s - loss: 0.0281 - accuracy: 0.9921\n",
      "Learning rate for epoch 4 is 9.999999747378752e-05\n",
      "938/938 [==============================] - 12s 13ms/step - loss: 0.0281 - accuracy: 0.9920\n",
      "Epoch 5/12\n",
      "937/938 [============================>.] - ETA: 0s - loss: 0.0249 - accuracy: 0.9932\n",
      "Learning rate for epoch 5 is 9.999999747378752e-05\n",
      "938/938 [==============================] - 11s 12ms/step - loss: 0.0249 - accuracy: 0.9932\n",
      "Epoch 6/12\n",
      "935/938 [============================>.] - ETA: 0s - loss: 0.0229 - accuracy: 0.9940\n",
      "Learning rate for epoch 6 is 9.999999747378752e-05\n",
      "938/938 [==============================] - 12s 13ms/step - loss: 0.0229 - accuracy: 0.9940\n",
      "Epoch 7/12\n",
      "934/938 [============================>.] - ETA: 0s - loss: 0.0214 - accuracy: 0.9942\n",
      "Learning rate for epoch 7 is 9.999999747378752e-05\n",
      "938/938 [==============================] - 12s 13ms/step - loss: 0.0214 - accuracy: 0.9942\n",
      "Epoch 8/12\n",
      "934/938 [============================>.] - ETA: 0s - loss: 0.0185 - accuracy: 0.9956\n",
      "Learning rate for epoch 8 is 9.999999747378752e-06\n",
      "938/938 [==============================] - 12s 13ms/step - loss: 0.0185 - accuracy: 0.9955\n",
      "Epoch 9/12\n",
      "936/938 [============================>.] - ETA: 0s - loss: 0.0182 - accuracy: 0.9958\n",
      "Learning rate for epoch 9 is 9.999999747378752e-06\n",
      "938/938 [==============================] - 12s 13ms/step - loss: 0.0182 - accuracy: 0.9958\n",
      "Epoch 10/12\n",
      "935/938 [============================>.] - ETA: 0s - loss: 0.0181 - accuracy: 0.9956\n",
      "Learning rate for epoch 10 is 9.999999747378752e-06\n",
      "938/938 [==============================] - 12s 13ms/step - loss: 0.0180 - accuracy: 0.9956\n",
      "Epoch 11/12\n",
      "937/938 [============================>.] - ETA: 0s - loss: 0.0179 - accuracy: 0.9957\n",
      "Learning rate for epoch 11 is 9.999999747378752e-06\n",
      "938/938 [==============================] - 12s 13ms/step - loss: 0.0179 - accuracy: 0.9957\n",
      "Epoch 12/12\n",
      "934/938 [============================>.] - ETA: 0s - loss: 0.0176 - accuracy: 0.9958\n",
      "Learning rate for epoch 12 is 9.999999747378752e-06\n",
      "938/938 [==============================] - 11s 12ms/step - loss: 0.0177 - accuracy: 0.9958\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f4b7bf97748>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 训练模型\n",
    "model.fit(train_dataset, epochs=12, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint\t\t     ckpt_4.data-00000-of-00001\n",
      "ckpt_10.data-00000-of-00001  ckpt_4.index\n",
      "ckpt_10.index\t\t     ckpt_5.data-00000-of-00001\n",
      "ckpt_11.data-00000-of-00001  ckpt_5.index\n",
      "ckpt_11.index\t\t     ckpt_6.data-00000-of-00001\n",
      "ckpt_12.data-00000-of-00001  ckpt_6.index\n",
      "ckpt_12.index\t\t     ckpt_7.data-00000-of-00001\n",
      "ckpt_1.data-00000-of-00001   ckpt_7.index\n",
      "ckpt_1.index\t\t     ckpt_8.data-00000-of-00001\n",
      "ckpt_2.data-00000-of-00001   ckpt_8.index\n",
      "ckpt_2.index\t\t     ckpt_9.data-00000-of-00001\n",
      "ckpt_3.data-00000-of-00001   ckpt_9.index\n",
      "ckpt_3.index\n"
     ]
    }
   ],
   "source": [
    "!ls {checkpoint_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    157/Unknown - 3s 18ms/step - loss: 0.0405 - accuracy: 0.9865Eval loss: 0.040537810085080325, Eval Accuracy: 0.9865000247955322\n"
     ]
    }
   ],
   "source": [
    "# 评估模型\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "eval_loss, eval_acc = model.evaluate(eval_dataset)\n",
    "\n",
    "print('Eval loss: {}, Eval Accuracy: {}'.format(eval_loss, eval_acc))\n",
    "\n",
    "# 查看tensorboard\n",
    "# tensorboard --logdir=path/to/log-directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0109 16:32:05.694178 139963976697664 deprecation.py:323] From /usr/local/python3/lib/python3.6/site-packages/tensorflow/python/saved_model/signature_def_utils_impl.py:253: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n",
      "W0109 16:32:05.696126 139963976697664 export_utils.py:182] Export includes no default signature!\n",
      "W0109 16:32:06.089147 139963976697664 export_utils.py:182] Export includes no default signature!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    157/Unknown - 2s 14ms/step - loss: 0.0405 - accuracy: 0.9865Eval loss: 0.040538180162714925, Eval Accuracy: 0.9865000247955322\n",
      "    157/Unknown - 2s 13ms/step - loss: 0.0405 - accuracy: 0.9865Eval loss: 0.040537810085080325, Eval Accuracy: 0.9865000247955322\n"
     ]
    }
   ],
   "source": [
    "# 导出为 SavedModel\n",
    "# 将图形和变量导出为与平台无关的 SavedModel 格式。 保存模型后，可以在有或没有 scope 的情况下加载模型。\n",
    "\n",
    "path = 'saved_model/'\n",
    "tf.keras.experimental.export_saved_model(model, path)\n",
    "\n",
    "# ******** 无须strategy.scope ************\n",
    "unreplicated_model = tf.keras.experimental.load_from_saved_model(path)\n",
    "\n",
    "unreplicated_model.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "eval_loss, eval_acc = unreplicated_model.evaluate(eval_dataset)\n",
    "\n",
    "print('Eval loss: {}, Eval Accuracy: {}'.format(eval_loss, eval_acc))\n",
    "\n",
    "# ******** 有须strategy.scope ************\n",
    "with strategy.scope():\n",
    "  replicated_model = tf.keras.experimental.load_from_saved_model(path)\n",
    "  replicated_model.compile(loss='sparse_categorical_crossentropy',\n",
    "                           optimizer=tf.keras.optimizers.Adam(),\n",
    "                           metrics=['accuracy'])\n",
    "\n",
    "  eval_loss, eval_acc = replicated_model.evaluate(eval_dataset)\n",
    "  print ('Eval loss: {}, Eval Accuracy: {}'.format(eval_loss, eval_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.使用自定义训练循环进行分布式训练\n",
    "\n",
    "tf.distribute.MirroredStrategy 策略是如何运作的？\n",
    "\n",
    "- 所有变量和模型图都复制在副本上。\n",
    "- 输入都均匀分布在副本中。\n",
    "- 每个副本在收到输入后计算输入的损失和梯度。\n",
    "- 通过求和，每一个副本上的梯度都能同步。\n",
    "- 同步后，每个副本上的复制的变量都可以同样更新。\n",
    "\n",
    "[回到目录](#分布式训练)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0-beta1\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "# 导入 TensorFlow\n",
    "import tensorflow as tf\n",
    "\n",
    "# 帮助库\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "print(tf.__version__)\n",
    "\n",
    "\n",
    "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
    "\n",
    "# 向数组添加维度 -> 新的维度 == (28, 28, 1)\n",
    "# 我们这样做是因为我们模型中的第一层是卷积层，而且它需要一个四维的输入 (批大小, 高, 宽, 通道). 批大小维度稍后将添加。\n",
    "train_images = train_images[..., None]\n",
    "test_images = test_images[..., None]\n",
    "\n",
    "# 获取[0,1]范围内的图像。\n",
    "train_images = train_images / np.float32(255)\n",
    "test_images = test_images / np.float32(255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0110 09:21:26.471938 140554294826816 cross_device_ops.py:1168] There is non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of devices: 1\n"
     ]
    }
   ],
   "source": [
    "# 准备策略和数据\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "print ('Number of devices: {}'.format(strategy.num_replicas_in_sync))\n",
    "\n",
    "BUFFER_SIZE = len(train_images)\n",
    "\n",
    "BATCH_SIZE_PER_REPLICA = 64\n",
    "GLOBAL_BATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync\n",
    "\n",
    "EPOCHS = 10\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels)).shuffle(BUFFER_SIZE).batch(GLOBAL_BATCH_SIZE) \n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_images, test_labels)).batch(GLOBAL_BATCH_SIZE) \n",
    "\n",
    "train_dist_dataset = strategy.experimental_distribute_dataset(train_dataset)\n",
    "test_dist_dataset = strategy.experimental_distribute_dataset(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "  model = tf.keras.Sequential([\n",
    "      tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
    "      tf.keras.layers.MaxPooling2D(),\n",
    "      tf.keras.layers.Conv2D(64, 3, activation='relu'),\n",
    "      tf.keras.layers.MaxPooling2D(),\n",
    "      tf.keras.layers.Flatten(),\n",
    "      tf.keras.layers.Dense(64, activation='relu'),\n",
    "      tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "\n",
    "  return model\n",
    "\n",
    "# 创建检查点目录以存储检查点。\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[定义损失函数](https://www.tensorflow.org/tutorials/distribute/custom_training#%E5%AE%9A%E4%B9%89%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0)\n",
    "\n",
    "通常，在一台只有一个 GPU / CPU 的机器上，损失需要除去输入批量中的示例数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "  # 将减少设置为“无”，以便我们可以在之后进行这个减少并除以全局批量大小。\n",
    "  loss_object = tf.keras.losses.SparseCategoricalCrossentropy(reduction=tf.keras.losses.Reduction.NONE)\n",
    "  # 或者使用 loss_fn = tf.keras.losses.sparse_categorical_crossentropy\n",
    "  def compute_loss(labels, predictions):\n",
    "    per_example_loss = loss_object(labels, predictions)\n",
    "    return tf.nn.compute_average_loss(per_example_loss, global_batch_size=GLOBAL_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[定义衡量指标以跟踪损失和准确性]()\n",
    "\n",
    "这些指标可以跟踪测试的损失，训练和测试的准确性。 可使用.result（）随时获取累积的统计信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "  test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "\n",
    "  train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "  test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.5166115164756775, Accuracy: 81.48666381835938, Test Loss: 0.43088409304618835, Test Accuracy: 85.05999755859375\n",
      "Epoch 2, Loss: 0.3407503068447113, Accuracy: 87.63166809082031, Test Loss: 0.35095396637916565, Test Accuracy: 87.25\n",
      "Epoch 3, Loss: 0.29075008630752563, Accuracy: 89.33499908447266, Test Loss: 0.31871673464775085, Test Accuracy: 88.41999816894531\n",
      "Epoch 4, Loss: 0.25969064235687256, Accuracy: 90.49500274658203, Test Loss: 0.2988489270210266, Test Accuracy: 89.26000213623047\n",
      "Epoch 5, Loss: 0.23618030548095703, Accuracy: 91.2933349609375, Test Loss: 0.2877892255783081, Test Accuracy: 89.56999969482422\n",
      "Epoch 6, Loss: 0.2153063714504242, Accuracy: 92.12833404541016, Test Loss: 0.2837090492248535, Test Accuracy: 89.94000244140625\n",
      "Epoch 7, Loss: 0.1971343457698822, Accuracy: 92.81999969482422, Test Loss: 0.28046807646751404, Test Accuracy: 90.1500015258789\n",
      "Epoch 8, Loss: 0.18028132617473602, Accuracy: 93.47166442871094, Test Loss: 0.274112731218338, Test Accuracy: 90.48999786376953\n",
      "Epoch 9, Loss: 0.16448892652988434, Accuracy: 94.04499816894531, Test Loss: 0.27102038264274597, Test Accuracy: 90.58999633789062\n",
      "Epoch 10, Loss: 0.1501585841178894, Accuracy: 94.60333251953125, Test Loss: 0.2816644310951233, Test Accuracy: 90.62999725341797\n"
     ]
    }
   ],
   "source": [
    "# 必须在`strategy.scope`下创建模型和优化器。\n",
    "with strategy.scope():\n",
    "  model = create_model()\n",
    "\n",
    "  optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "  checkpoint = tf.train.Checkpoint(optimizer=optimizer, model=model)\n",
    "\n",
    "with strategy.scope():\n",
    "  def train_step(inputs):\n",
    "    images, labels = inputs\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "      predictions = model(images, training=True)\n",
    "      loss = compute_loss(labels, predictions)\n",
    "\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    train_accuracy.update_state(labels, predictions)\n",
    "    return loss \n",
    "\n",
    "  def test_step(inputs):\n",
    "    images, labels = inputs\n",
    "\n",
    "    predictions = model(images, training=False)\n",
    "    t_loss = loss_object(labels, predictions)\n",
    "\n",
    "    test_loss.update_state(t_loss)\n",
    "    test_accuracy.update_state(labels, predictions)\n",
    "    \n",
    "\n",
    "with strategy.scope():\n",
    "  # `experimental_run_v2`将复制提供的计算并使用分布式输入运行它。\n",
    "  @tf.function\n",
    "  def distributed_train_step(dataset_inputs):\n",
    "    per_replica_losses = strategy.experimental_run_v2(train_step,args=(dataset_inputs,))\n",
    "    return strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses, axis=None)\n",
    " \n",
    "  @tf.function\n",
    "  def distributed_test_step(dataset_inputs):\n",
    "    return strategy.experimental_run_v2(test_step, args=(dataset_inputs,))\n",
    "\n",
    "  for epoch in range(EPOCHS):\n",
    "    # 训练循环\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "    for x in train_dist_dataset:\n",
    "      total_loss += distributed_train_step(x)\n",
    "      num_batches += 1\n",
    "    train_loss = total_loss / num_batches\n",
    "\n",
    "    # 测试循环\n",
    "    for x in test_dist_dataset:\n",
    "      distributed_test_step(x)\n",
    "\n",
    "    if epoch % 2 == 0:\n",
    "      checkpoint.save(checkpoint_prefix)\n",
    "\n",
    "    template = (\"Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, \"\n",
    "                \"Test Accuracy: {}\")\n",
    "    print (template.format(epoch+1, train_loss,\n",
    "                           train_accuracy.result()*100, test_loss.result(),\n",
    "                           test_accuracy.result()*100))\n",
    "\n",
    "    test_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    test_accuracy.reset_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上示例中需要注意的事项：\n",
    "\n",
    "我们使用`for x in ...`迭代构造`train_dist_dataset`和`test_dist_dataset`。\n",
    "\n",
    "缩放损失是`distributed_train_step`的返回值。 这个值会在各个副本使用`tf.distribute.Strategy.reduce`的时候合并，然后通过`tf.distribute.Strategy.reduce`叠加各个返回值来跨批次。\n",
    "\n",
    "在执行`tf.distribute.Strategy.experimental_run_v2`时，tf.keras.Metrics应在train_step和test_step中更新。\n",
    "\n",
    "`tf.distribute.Strategy.experimental_run_v2`返回策略中每个本地副本的结果，并且有多种方法可以处理此结果。 可以执行`tf.distribute.Strategy.reduce`来获取汇总值。 还可以执行`tf.distribute.Strategy.experimental_local_results`来获取每个本地副本中结果中包含的值列表。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[恢复最新的检查点并进行测试]()\n",
    "\n",
    "一个模型使用了tf.distribute.Strategy的检查点可以使用策略或者不使用策略进行恢复。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy after restoring the saved model without strategy: 90.58999633789062\n"
     ]
    }
   ],
   "source": [
    "eval_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='eval_accuracy')\n",
    "\n",
    "new_model = create_model()\n",
    "new_optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_images, test_labels)).batch(GLOBAL_BATCH_SIZE)\n",
    "\n",
    "@tf.function\n",
    "def eval_step(images, labels):\n",
    "  predictions = new_model(images, training=False)\n",
    "  eval_accuracy(labels, predictions)\n",
    "    \n",
    "checkpoint = tf.train.Checkpoint(optimizer=new_optimizer, model=new_model)\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "for images, labels in test_dataset:\n",
    "  eval_step(images, labels)\n",
    "\n",
    "print ('Accuracy after restoring the saved model without strategy: {}'.format(eval_accuracy.result()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[迭代一个数据集的替代方法](https://www.tensorflow.org/tutorials/distribute/custom_training#%E8%BF%AD%E4%BB%A3%E4%B8%80%E4%B8%AA%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9A%84%E6%9B%BF%E4%BB%A3%E6%96%B9%E6%B3%95)\n",
    "\n",
    "- 使用迭代器\n",
    "- 在 tf.function 中迭代\n",
    "- [跟踪副本中的训练的损失](https://www.tensorflow.org/tutorials/distribute/custom_training#%E8%B7%9F%E8%B8%AA%E5%89%AF%E6%9C%AC%E4%B8%AD%E7%9A%84%E8%AE%AD%E7%BB%83%E7%9A%84%E6%8D%9F%E5%A4%B1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 0.11914627254009247, Accuracy: 96.09375\n",
      "Epoch 10, Loss: 0.0768473818898201, Accuracy: 97.65625\n",
      "Epoch 10, Loss: 0.048872582614421844, Accuracy: 99.0625\n",
      "Epoch 10, Loss: 0.03392244130373001, Accuracy: 99.6875\n",
      "Epoch 10, Loss: 0.025061551481485367, Accuracy: 100.0\n",
      "Epoch 10, Loss: 0.020258614793419838, Accuracy: 100.0\n",
      "Epoch 10, Loss: 0.016797250136733055, Accuracy: 100.0\n",
      "Epoch 10, Loss: 0.01446988433599472, Accuracy: 100.0\n",
      "Epoch 10, Loss: 0.012694062665104866, Accuracy: 100.0\n",
      "Epoch 10, Loss: 0.011253146454691887, Accuracy: 100.0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "如果你想要迭代一个已经给定步骤数量而不需要整个遍历的数据集，你可以创建一个迭代器并在迭代器上调用iter和显式调用next。 \n",
    "可以选择在 tf.function 内部和外部迭代数据集。 这是一个小片段，演示了使用迭代器在 tf.function 外部迭代数据集。\n",
    "\"\"\"\n",
    "with strategy.scope():\n",
    "  for _ in range(EPOCHS):\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "    train_iter = iter(train_dist_dataset)\n",
    "\n",
    "    for _ in range(10):\n",
    "      total_loss += distributed_train_step(next(train_iter))\n",
    "      num_batches += 1\n",
    "    average_train_loss = total_loss / num_batches\n",
    "\n",
    "    template = (\"Epoch {}, Loss: {}, Accuracy: {}\")\n",
    "    print (template.format(epoch+1, average_train_loss, train_accuracy.result()*100))\n",
    "    train_accuracy.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.13831807672977448, Accuracy: 95.05332946777344\n",
      "Epoch 2, Loss: 0.14503231644630432, Accuracy: 94.60832977294922\n",
      "Epoch 3, Loss: 0.12937283515930176, Accuracy: 95.13833618164062\n",
      "Epoch 4, Loss: 0.11664383113384247, Accuracy: 95.71333312988281\n",
      "Epoch 5, Loss: 0.10745175182819366, Accuracy: 95.99833679199219\n",
      "Epoch 6, Loss: 0.0979103222489357, Accuracy: 96.32499694824219\n",
      "Epoch 7, Loss: 0.09036801010370255, Accuracy: 96.62332916259766\n",
      "Epoch 8, Loss: 0.08281902223825455, Accuracy: 96.87999725341797\n",
      "Epoch 9, Loss: 0.07657422125339508, Accuracy: 97.18833923339844\n",
      "Epoch 10, Loss: 0.06852100044488907, Accuracy: 97.46333312988281\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "您还可以使用for x in ...构造在 tf.function 内部迭代整个输入train_dist_dataset，或者像上面那样创建迭代器。\n",
    "下面的例子演示了在 tf.function 中包装一个 epoch 并在功能内迭代train_dist_dataset。\n",
    "\"\"\"\n",
    "with strategy.scope():\n",
    "  @tf.function\n",
    "  def distributed_train_epoch(dataset):\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "    for x in dataset:\n",
    "      per_replica_losses = strategy.experimental_run_v2(train_step, args=(x,))\n",
    "      total_loss += strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses, axis=None)\n",
    "      num_batches += 1\n",
    "    return total_loss / tf.cast(num_batches, dtype=tf.float32)\n",
    "\n",
    "  for epoch in range(EPOCHS):\n",
    "    train_loss = distributed_train_epoch(train_dist_dataset)\n",
    "\n",
    "    template = (\"Epoch {}, Loss: {}, Accuracy: {}\")\n",
    "    print (template.format(epoch+1, train_loss, train_accuracy.result()*100))\n",
    "\n",
    "    train_accuracy.reset_states()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.使用Estimator进行多工作器训练\n",
    "\n",
    "[回到目录](#分布式训练)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0110 10:41:40.797106 139944007546688 cross_device_ops.py:1168] There is non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals \n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "tfds.disable_progress_bar()\n",
    "\n",
    "import os, json\n",
    "\n",
    "\"\"\"\n",
    "MultiWorkerMirroredStrategy 创建了每个设备中模型层里所有变量的拷贝，且是跨工作器的。\n",
    "其用到了 CollectiveOps，这是 TensorFlow 里的一种操作，用来整合梯度以及确保变量同步。\n",
    "\"\"\"\n",
    "strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()\n",
    "\n",
    "\"\"\"\n",
    "多工作器配置\n",
    "\"\"\"\n",
    "os.environ['TF_CONFIG'] = json.dumps({\n",
    "    'cluster': {\n",
    "        'worker': [\"localhost:12345\", \"localhost:23456\"]\n",
    "    },\n",
    "    'task': {'type': 'worker', 'index': 0}\n",
    "})\n",
    "\n",
    "\n",
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "def input_fn(mode, input_context=None):\n",
    "  datasets, info = tfds.load(name='mnist', with_info=True, as_supervised=True)\n",
    "  mnist_dataset = (datasets['train'] if mode == tf.estimator.ModeKeys.TRAIN else datasets['test'])  # 训练 or 测试\n",
    "\n",
    "  def scale(image, label):\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image /= 255\n",
    "    return image, label\n",
    "\n",
    "  if input_context:\n",
    "    mnist_dataset = mnist_dataset.shard(input_context.num_input_pipelines, input_context.input_pipeline_id)\n",
    "  return mnist_dataset.map(scale).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 1e-4\n",
    "def model_fn(features, labels, mode):\n",
    "    model = tf.keras.Sequential([\n",
    "      tf.keras.layers.Conv2D(32, 3, activation='relu', input_shape=(28, 28, 1)),\n",
    "      tf.keras.layers.MaxPooling2D(),\n",
    "      tf.keras.layers.Flatten(),\n",
    "      tf.keras.layers.Dense(64, activation='relu'),\n",
    "      tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    logits = model(features, training=False)\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        predictions = {'logits': logits}\n",
    "        return tf.estimator.EstimatorSpec(labels=labels, predictions=predictions)\n",
    "\n",
    "    optimizer = tf.compat.v1.train.GradientDescentOptimizer(learning_rate=LEARNING_RATE)\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "        from_logits=True, reduction=tf.keras.losses.Reduction.NONE)(labels, logits)\n",
    "    loss = tf.reduce_sum(loss) * (1. / BATCH_SIZE)\n",
    "    if mode == tf.estimator.ModeKeys.EVAL:\n",
    "        return tf.estimator.EstimatorSpec(mode, loss=loss)\n",
    "\n",
    "    return tf.estimator.EstimatorSpec(mode=mode, loss=loss,\n",
    "          train_op=optimizer.minimize(loss, tf.compat.v1.train.get_or_create_global_step()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0110 10:41:41.039667 139944007546688 distribute_coordinator.py:829] `eval_strategy` is not passed in. No distribution strategy will be used for evaluation.\n",
      "W0110 10:41:41.044196 139944007546688 cross_device_ops.py:1164] Some requested devices in `tf.distribute.Strategy` are not visible to TensorFlow: /job:worker/replica:0/task:0/device:CPU:0\n",
      "W0110 10:41:41.054024 139944007546688 cross_device_ops.py:1164] Some requested devices in `tf.distribute.Strategy` are not visible to TensorFlow: /job:worker/replica:0/task:0/device:CPU:0\n",
      "W0110 10:41:42.008032 139940201412352 deprecation.py:323] From /usr/local/python3/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py:1340: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0110 10:41:42.017798 139944007546688 monitored_session.py:347] Collective ops may deadlock with `save_checkpoints_secs` please use `save_checkpoint_steps` instead. Clearing `save_checkpoint_secs` and setting `save_checkpoint_steps` to 1000 now.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "在 RunConfig 中为 estimator 指明分布式策略，同时通过调用 tf.estimator.train_and_evaluate 训练和评估模型。\n",
    "此处指明 train_distribute 进行分布式训练。同样也可以指明 eval_distribute 来进行分布式评估。\n",
    "\"\"\"\n",
    "config = tf.estimator.RunConfig(train_distribute=strategy)\n",
    "\n",
    "classifier = tf.estimator.Estimator(model_fn=model_fn, model_dir='./multiworker', config=config)\n",
    "tf.estimator.train_and_evaluate(\n",
    "    classifier,\n",
    "    train_spec=tf.estimator.TrainSpec(input_fn=input_fn),\n",
    "    eval_spec=tf.estimator.EvalSpec(input_fn=input_fn)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[优化多工作器训练的性能]()\n",
    "\n",
    "增加单批次的大小： 此处的批次大小指的是每个 GPU 上的批次大小。通常来说，最大的批次大小应该适应 GPU 的内存大小。\n",
    "\n",
    "变量转换： 尽可能将变量转换成 tf.float。官方的 ResNet 模型包括了如何完成的样例。\n",
    "\n",
    "使用集群通信： MultiWorkerMirroredStrategy 提供了好几种集群通信的实现.\n",
    "- RING 实现了基于环状的集群，使用了 gRPC 作为跨主机通讯层。\n",
    "- NCCL 使用了 英伟达的 NCCL 来实现集群。\n",
    "- AUTO 将选择延后至运行时。\n",
    "\n",
    "集群实现的最优选择不仅基于 GPU 的数量和种类，也基于集群间的通信网络。**想要覆盖自动的选项，需要指明 MultiWorkerMirroredStrategy 的构造器里的 communication 参数**，例如让 communication=tf.distribute.experimental.CollectiveCommunication.NCCL "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.其他\n",
    "\n",
    "[回到目录](#分布式训练)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "★ 以下是使用 keras fit/compile 分布式策略的一些示例：\n",
    "\n",
    "使用tf.distribute.MirroredStrategy 训练 [Transformer](https://github.com/tensorflow/models/blob/master/official/transformer/v2/transformer_main.py) 的示例。\n",
    "\n",
    "使用tf.distribute.MirroredStrategy 训练 [NCF](https://github.com/tensorflow/models/blob/master/official/recommendation/ncf_keras_main.py) 的示例。\n",
    "\n",
    "使用tf.distribute.MirroredStrategy 训练 [MNIST](https://www.tensorflow.org/tutorials/distribute/keras) 的示例。\n",
    "\n",
    "使用tf.distribute.MirroredStrategy 训练 [ResNet50](https://github.com/tensorflow/models/blob/master/official/vision/image_classification/resnet_imagenet_main.py) 的示例。\n",
    "\n",
    "使用❤tf.distribute.MultiWorkerMirroredStrategy❤训练 [MNIST](https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras) 的示例。 \n",
    "\n",
    "使用tf.distribute.TPUStrategy 训练 [ResNet50](https://github.com/tensorflow/tpu/blob/master/models/experimental/resnet50_keras/resnet50_tf2.py) 的示例。\n",
    "\n",
    "\n",
    "★ 以下是一些使用自定义训练循环来分发策略的示例：\n",
    "\n",
    "使用tf.distribute.MirroredStrategy 训练 [DenseNet](https://github.com/tensorflow/examples/blob/master/tensorflow_examples/models/densenet/distributed_train.py)\n",
    "\n",
    "使用tf.distribute.MirroredStrategy 训练 [NMT](https://github.com/tensorflow/examples/blob/master/tensorflow_examples/models/nmt_with_attention/distributed_train.py)\n",
    "\n",
    "[BERT](https://github.com/tensorflow/models/blob/master/official/bert/run_classifier.py) 使用 MirroredStrategy 和TPUStrategy来训练的例子。 此示例对于了解如何在分发训练过程中如何载入一个检测点和定期生成检查点特别有帮助。\n",
    "\n",
    "[NCF](https://github.com/tensorflow/models/blob/master/official/recommendation/ncf_keras_main.py) 使用 MirroredStrategy 来启用 keras_use_ctl 标记。\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
